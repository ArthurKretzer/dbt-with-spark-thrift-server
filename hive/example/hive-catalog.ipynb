{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450929e8",
   "metadata": {},
   "source": [
    "# Hive Catalog\n",
    "\n",
    "This notebook provides an example of external table registration on Hive Metastore and Delta table interaction.\n",
    "\n",
    "You must provide a .env file with MinIO credentials and two tables as example, one in Parquet and the other in Delta Lake format:\n",
    "\n",
    "```\n",
    "MINIO_SECRET_KEY\n",
    "MINIO_ACCESS_KEY\n",
    "MINIO_ENDPOINT\n",
    "ROBOTS_PATH # Delta Table\n",
    "LEMOM_AREAS # Parquet Table\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd486bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark==3.5 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d257629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit this URI for your environment\n",
    "HIVE_URI = \"172.16.203.10:9083\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827785a-913a-4862-bce8-c8ea17ae845c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T11:59:38.038342Z",
     "iopub.status.busy": "2025-04-29T11:59:38.037751Z",
     "iopub.status.idle": "2025-04-29T11:59:52.901568Z",
     "shell.execute_reply": "2025-04-29T11:59:52.900378Z",
     "shell.execute_reply.started": "2025-04-29T11:59:38.038303Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "            \"--packages org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-spark_2.12:3.3.0 pyspark-shell\"\n",
    "        )\n",
    "\n",
    "app_name = \"Data Backend\"\n",
    "\n",
    "print(\"Initializing spark...\")\n",
    "print(os.getenv(\"MINIO_ACCESS_KEY\"))\n",
    "print(os.getenv(\"MINIO_ENDPOINT\"))\n",
    "spark = (\n",
    "    SparkSession.builder.appName(app_name)\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.access.key\",\n",
    "        os.getenv(\"MINIO_ACCESS_KEY\"),\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.secret.key\",\n",
    "        os.getenv(\"MINIO_SECRET_KEY\"),\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.endpoint\",\n",
    "        os.getenv(\"MINIO_ENDPOINT\"),\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.impl\",\n",
    "        \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"io.delta.sql.DeltaSparkSessionExtension\",\n",
    "    )\n",
    "    .config(\"spark.hive.metastore.uris\", \"thrift://172.16.203.10:9083\") \\\n",
    "    .config(\"spark.hive.metastore.schema.verification\", \"false\") \\\n",
    "    .config(\"spark.sql.hive.thriftServer.singleSession\", \"false\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config('spark.sql.warehouse.dir', \"s3a://warehouse/delta/\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"delta.autoOptimize.optimizeWrite\", \"true\") \\\n",
    "    .config(\"delta.autoOptimize.autoCompact\", \"true\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8953066-44a0-4a7d-b131-ce4f3ac73d96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T12:01:11.330385Z",
     "iopub.status.busy": "2025-04-29T12:01:11.329633Z",
     "iopub.status.idle": "2025-04-29T12:01:11.358017Z",
     "shell.execute_reply": "2025-04-29T12:01:11.354911Z",
     "shell.execute_reply.started": "2025-04-29T12:01:11.330341Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"robots\": {\n",
    "        \"path\" : os.getenv(\"ROBOTS_PATH\"),\n",
    "        \"type\" : \"delta\"\n",
    "    },\n",
    "    \"lemom_areas\": {\n",
    "        \"path\" : os.getenv(\"LEMOM_AREAS\"),\n",
    "        \"type\" : \"parquet\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def load_spark_tables(spark_session: SparkSession):\n",
    "    \"\"\"\n",
    "    Loads Parquet files into Spark temporary views for Silver layer tables.\n",
    "\n",
    "    This function iterates over a predefined dictionary of datasets and\n",
    "    their corresponding file paths,\n",
    "    reads each Parquet file into a Spark DataFrame, and registers the DataFrame\n",
    "    as a temporary view\n",
    "    with the dataset name as the view name.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example Usage:\n",
    "        ```python\n",
    "        spark_api = SparkAPI()\n",
    "        load_spark_tables(spark_api.spark)\n",
    "        ```\n",
    "\n",
    "    Notes:\n",
    "        - The `datasets` dictionary must be defined beforehand, where keys\n",
    "            are dataset names and values are file paths.\n",
    "        - Each Parquet file is read using Spark and registered as a temporary\n",
    "            view for SQL operations.\n",
    "        - Ensure that the `datasets` dictionary and the Parquet files exist\n",
    "            before calling this function.\n",
    "    \"\"\"\n",
    "    for dataset_name, settings in datasets.items():\n",
    "        spark_session.sql(\"CREATE DATABASE IF NOT EXISTS delta;\")\n",
    "        spark_session.sql(\"CREATE DATABASE IF NOT EXISTS parquet;\")\n",
    "        print(f\"Registering table {dataset_name} of type {settings['type']} in path {settings['path']}\")\n",
    "        spark_session.sql(\n",
    "            f\"\"\"\n",
    "            CREATE EXTERNAL TABLE IF NOT EXISTS {settings['type']}.{dataset_name}\n",
    "            USING {settings[\"type\"].upper()}\n",
    "            LOCATION '{settings['path']}';\n",
    "        \"\"\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0417de10-6b37-49b6-a3bc-15d5e6c42841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T12:01:42.507885Z",
     "iopub.status.busy": "2025-04-29T12:01:42.503328Z",
     "iopub.status.idle": "2025-04-29T12:01:42.720793Z",
     "shell.execute_reply": "2025-04-29T12:01:42.719265Z",
     "shell.execute_reply.started": "2025-04-29T12:01:42.507836Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897dd3a-9f62-44af-a644-e65489ed0c2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T12:01:14.283167Z",
     "iopub.status.busy": "2025-04-29T12:01:14.282862Z",
     "iopub.status.idle": "2025-04-29T12:01:14.660521Z",
     "shell.execute_reply": "2025-04-29T12:01:14.659650Z",
     "shell.execute_reply.started": "2025-04-29T12:01:14.283146Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES IN delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee0047-6be7-47d2-8bbf-5ae997acc1c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T12:01:49.651755Z",
     "iopub.status.busy": "2025-04-29T12:01:49.651321Z",
     "iopub.status.idle": "2025-04-29T12:01:50.084252Z",
     "shell.execute_reply": "2025-04-29T12:01:50.083035Z",
     "shell.execute_reply.started": "2025-04-29T12:01:49.651721Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES IN parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f90fa-1e7b-4a5c-8823-d4cb8d405cf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T12:01:15.254831Z",
     "iopub.status.busy": "2025-04-29T12:01:15.254440Z",
     "iopub.status.idle": "2025-04-29T12:01:27.762162Z",
     "shell.execute_reply": "2025-04-29T12:01:27.761194Z",
     "shell.execute_reply.started": "2025-04-29T12:01:15.254802Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_spark_tables(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747819d-24f6-4293-803c-3c1f495277df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T18:17:12.902875Z",
     "iopub.status.busy": "2025-04-28T18:17:12.901688Z",
     "iopub.status.idle": "2025-04-28T18:17:13.435240Z",
     "shell.execute_reply": "2025-04-28T18:17:13.432913Z",
     "shell.execute_reply.started": "2025-04-28T18:17:12.902827Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE delta.robots_uph\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51343c-515b-4598-998e-001c7bd304d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T18:17:17.013176Z",
     "iopub.status.busy": "2025-04-28T18:17:17.012775Z",
     "iopub.status.idle": "2025-04-28T18:17:40.988664Z",
     "shell.execute_reply": "2025-04-28T18:17:40.987532Z",
     "shell.execute_reply.started": "2025-04-28T18:17:17.013148Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE DETAIL parquet.lemom_areas\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
