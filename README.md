# Open Data Lakehouse Framework with Hive Metastore, Spark Thrift Server, and DBT

This project provides an open-source framework to build and experiment with modern data lakehouse architectures using:

- **Hive Metastore** for schema management  
- **Apache Spark Thrift Server** for distributed query execution  
- **DBT** for SQL-based transformations and model management  
- **MinIO** as a local S3-compatible object store (for development purposes)  

It is designed to help developers and data engineers bootstrap lakehouse stacks with open-source tools for local experimentation, prototyping, and education.

---

## üìö Summary

- [Open Data Lakehouse Framework with Hive Metastore, Spark Thrift Server, and DBT](#open-data-lakehouse-framework-with-hive-metastore-spark-thrift-server-and-dbt)
  - [üìö Summary](#-summary)
  - [üå≤ Repository Structure](#-repository-structure)
  - [üöÄ Quick Start (Docker)](#-quick-start-docker)
  - [‚öôÔ∏è Python Environment Setup (UV)](#Ô∏è-python-environment-setup-uv)
  - [üîß DBT Commands](#-dbt-commands)
  - [üß™ Connect to Spark Thrift Server with Beeline](#-connect-to-spark-thrift-server-with-beeline)
    - [Iceberg](#iceberg)
    - [Delta Lake](#delta-lake)
  - [Gotchas](#gotchas)
  - [üìù License](#-license)

---

## üå≤ Repository Structure

```text
.
‚îú‚îÄ‚îÄ hive/                       # Hive configurations and examples
‚îÇ   ‚îî‚îÄ‚îÄ example/
‚îú‚îÄ‚îÄ logs/                       # Spark event logs and Thrift logs
‚îú‚îÄ‚îÄ minio/                      # MinIO deployment configs
‚îú‚îÄ‚îÄ parking_lot/               # Experimental and alternative setups
‚îÇ   ‚îú‚îÄ‚îÄ external_master/        # Spark master externalized setup
‚îÇ   ‚îú‚îÄ‚îÄ iceberg/                # Iceberg variant (future support)
‚îÇ   ‚îî‚îÄ‚îÄ pure_hive/              # Hive-only setup with DBT
‚îÇ       ‚îî‚îÄ‚îÄ config/
‚îú‚îÄ‚îÄ spark_dbt_project/          # Main DBT project
‚îÇ   ‚îú‚îÄ‚îÄ analyses/               # Ad-hoc queries
‚îÇ   ‚îú‚îÄ‚îÄ logs/                   # DBT logs
‚îÇ   ‚îú‚îÄ‚îÄ macros/                 # Custom Jinja macros
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # DBT models (SQL transformations)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ example/
‚îÇ   ‚îú‚îÄ‚îÄ seeds/                  # CSV files loaded into tables
‚îÇ   ‚îú‚îÄ‚îÄ snapshots/              # Slowly changing dimensions (SCD) tracking
‚îÇ   ‚îú‚îÄ‚îÄ target/                 # DBT build artifacts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ compiled/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spark_dbt_project/models/example/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ run/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ spark_dbt_project/models/example/
‚îÇ   ‚îî‚îÄ‚îÄ tests/                  # Custom schema/data tests
‚îî‚îÄ‚îÄ thrift_server/              # Spark Thrift Server container configs
    ‚îî‚îÄ‚îÄ config/
```

---

## üöÄ Quick Start (Docker)

Make sure Docker and Docker Compose are installed.

```bash
cp minio/.env-example minio/.env && cp thrift_server/.env-example thrift_server/.env && cp hive/.env-example hive/.env
docker-compose up -d
```

This command will:

- Start the Spark Thrift Server
- Start Hive Metastore
- Launch MinIO (for S3-like object storage)
- You can now connect to the Thrift Server via localhost:10000 using DBT or any JDBC/ODBC client.

---

## ‚öôÔ∏è Python Environment Setup (UV)

We use [Astral UV](https://github.com/astral-sh/uv) for Python version management and dependency isolation.

1. Install ``uv`` if you don't have it:

    ```bash
    curl -LsSf https://astral.sh/uv/install.sh | sh
    ```

2. Install the correct Python version:

    This project uses Python 3.13. To install it via UV:

    ```bash
    uv venv --python 3.13
    ```

    Then activate the environment:

    ```bash
    source .venv/bin/activate
    ```

---

## üîß DBT Commands

Once your containers are running and environment is ready:

Debug the DBT connection:

```bash
dbt debug
```

Run all models:

```bash
dbt run --target delta_thrift --select path:models/delta
dbt run --target iceberg_thrift --select path:models/iceberg
```

Rebuild from scratch (drops and recreates tables):

```bash
dbt run --full-refresh --target delta_thrift --select path:models/delta
dbt run --full-refresh --target iceberg_thrift --select path:models/iceberg
```

You can edit or add models inside ``spark_dbt_project/models/`` and rerun ``dbt run``.

---

## üß™ Connect to Spark Thrift Server with Beeline

You can manually test the Spark Thrift Server connection using Beeline, the JDBC CLI for Hive and Spark SQL.

If any errors occur, DBT will also not work.

### Iceberg

1. Open a terminal inside the Spark Thrift Server container:

    ```bash
    docker exec -it spark-thrift-iceberg /bin/bash
    ```

2. Start Beeline:

    ```bash
    /opt/spark/bin/beeline
    ```

    You should now see the ``beeline>`` prompt.

3. Connect to the Spark Thrift Server:

    ```bash
    !connect jdbc:hive2://localhost:10000
    ```

    >If authentication is not required, just press Enter when prompted for username and password.

4. Run Spark Thrift Server tests:

    ```sql
    CREATE TABLE default.sample_iceberg (
        id INT,
        name STRING
    )
    USING iceberg
    LOCATION 's3a://lakehouse/iceberg/sample_iceberg';

    INSERT INTO default.sample_iceberg VALUES (1,'Alice'),(2,'Bob');

    SELECT * FROM default.sample_iceberg;

    SHOW CATALOGS;
    ```

### Delta Lake

1. Open a terminal inside the Spark Thrift Server container:

    ```bash
    docker exec -it spark-thrift-delta /bin/bash
    ```

2. Start Beeline:

    ```bash
    /opt/spark/bin/beeline
    ```

    You should now see the ``beeline>`` prompt.

3. Connect to the Spark Thrift Server:

    ```bash
    !connect jdbc:hive2://localhost:10000
    ```

    >If authentication is not required, just press Enter when prompted for username and password.

4. Run Spark Thrift Server tests:

    ```sql
    CREATE TABLE default.sample_delta (
        id INT,
        name STRING
    )
    USING delta
    LOCATION 's3a://lakehouse/delta/sample_delta';

    INSERT INTO default.sample_delta VALUES (1,'Alice'),(2,'Bob');

    SELECT * FROM default.sample_delta;

    SHOW CATALOGS;
    ```

---

## Gotchas

- Although we can configure a spark session to have two catalogs (delta and iceberg) and a thrift server as such, we can't configure DBT to accept the catalog specification (``catalog.schema.table``) as stated [in the docs](https://docs.getdbt.com/reference/resource-configs/spark-configs#always-schema-never-database).

---

## üìù License

This project is open-source and licensed under the Apache 2.0 License
