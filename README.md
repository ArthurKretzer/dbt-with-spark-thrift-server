# Open Data Lakehouse Framework with Hive Metastore, Spark Thrift Server, and DBT

This project provides an open-source framework to build and experiment with modern data lakehouse architectures using:

- **Hive Metastore** for schema management  
- **Apache Spark Thrift Server** for distributed query execution  
- **DBT** for SQL-based transformations and model management  
- **MinIO** as a local S3-compatible object store (for development purposes)  

It is designed to help developers and data engineers bootstrap lakehouse stacks with open-source tools for local experimentation, prototyping, and education.

---

## ğŸ“š Summary

- [Open Data Lakehouse Framework with Hive Metastore, Spark Thrift Server, and DBT](#open-data-lakehouse-framework-with-hive-metastore-spark-thrift-server-and-dbt)
  - [ğŸ“š Summary](#-summary)
  - [ğŸŒ² Repository Structure](#-repository-structure)
  - [ğŸš€ Quick Start (Docker)](#-quick-start-docker)
  - [âš™ï¸ Python Environment Setup (UV)](#ï¸-python-environment-setup-uv)
  - [ğŸ”§ DBT Commands](#-dbt-commands)
  - [ğŸ§ª Connect to Spark Thrift Server with Beeline](#-connect-to-spark-thrift-server-with-beeline)
  - [ğŸ“ License](#-license)

---

## ğŸŒ² Repository Structure

```text
.
â”œâ”€â”€ hive/                       # Hive configurations and examples
â”‚   â””â”€â”€ example/
â”œâ”€â”€ logs/                       # Spark event logs and Thrift logs
â”œâ”€â”€ minio/                      # MinIO deployment configs
â”œâ”€â”€ parking_lot/               # Experimental and alternative setups
â”‚   â”œâ”€â”€ external_master/        # Spark master externalized setup
â”‚   â”œâ”€â”€ iceberg/                # Iceberg variant (future support)
â”‚   â””â”€â”€ pure_hive/              # Hive-only setup with DBT
â”‚       â””â”€â”€ config/
â”œâ”€â”€ spark_dbt_project/          # Main DBT project
â”‚   â”œâ”€â”€ analyses/               # Ad-hoc queries
â”‚   â”œâ”€â”€ logs/                   # DBT logs
â”‚   â”œâ”€â”€ macros/                 # Custom Jinja macros
â”‚   â”œâ”€â”€ models/                 # DBT models (SQL transformations)
â”‚   â”‚   â””â”€â”€ example/
â”‚   â”œâ”€â”€ seeds/                  # CSV files loaded into tables
â”‚   â”œâ”€â”€ snapshots/              # Slowly changing dimensions (SCD) tracking
â”‚   â”œâ”€â”€ target/                 # DBT build artifacts
â”‚   â”‚   â”œâ”€â”€ compiled/
â”‚   â”‚   â”‚   â””â”€â”€ spark_dbt_project/models/example/
â”‚   â”‚   â””â”€â”€ run/
â”‚   â”‚       â””â”€â”€ spark_dbt_project/models/example/
â”‚   â””â”€â”€ tests/                  # Custom schema/data tests
â””â”€â”€ thrift_server/              # Spark Thrift Server container configs
    â””â”€â”€ config/
```

---

## ğŸš€ Quick Start (Docker)

Make sure Docker and Docker Compose are installed.

```bash
cp minio/.env-example minio/.env && cp thrift_server/.env-example thrift_server/.env && cp hive/.env-example hive/.env
docker-compose up -d
```

This command will:

- Start the Spark Thrift Server
- Start Hive Metastore
- Launch MinIO (for S3-like object storage)
- You can now connect to the Thrift Server via localhost:10000 using DBT or any JDBC/ODBC client.

---

## âš™ï¸ Python Environment Setup (UV)

We use [Astral UV](https://github.com/astral-sh/uv) for Python version management and dependency isolation.

1. Install ``uv`` if you don't have it:

    ```bash
    curl -LsSf https://astral.sh/uv/install.sh | sh
    ```

2. Install the correct Python version:

    This project uses Python 3.13. To install it via UV:

    ```bash
    uv venv --python 3.13
    ```

    Then activate the environment:

    ```bash
    source .venv/bin/activate
    ```

---

## ğŸ”§ DBT Commands

Once your containers are running and environment is ready:

Debug the DBT connection:

```bash
dbt debug
```

Run all models:

```bash
dbt run
```

Rebuild from scratch (drops and recreates tables):

```bash
dbt run --full-refresh
```

You can edit or add models inside ``spark_dbt_project/models/`` and rerun ``dbt run``.

---

## ğŸ§ª Connect to Spark Thrift Server with Beeline

You can manually test the Spark Thrift Server connection using Beeline, the JDBC CLI for Hive and Spark SQL.

1. Open a terminal inside the Spark Thrift Server container:

    ```bash
    docker exec -it spark-thrift /bin/bash
    ```

2. Start Beeline:

    ```bash
    /opt/spark/bin/beeline
    ```

    You should now see the ``beeline>`` prompt.

3. Connect to the Spark Thrift Server:

    ```bash
    !connect jdbc:hive2://localhost:10000
    ```

    >>If authentication is not required, just press Enter when prompted for username and password.

4. Run a simple query:

    ```bash
    select * from default.my_first_dbt_model;
    ```

---

## ğŸ“ License

This project is open-source and licensed under the Apache 2.0 License
